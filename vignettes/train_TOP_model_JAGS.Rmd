---
title: "Train TOP model"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Train TOP model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Load TOP R package
---------------------
```{r load-TOP-package, eval = FALSE}
library(TOP)
```


Prepare training data
---------------------

**Step 1: Prepare training data for each TF in each cell type**

For each TF in each cell type, prepare training data for candidate binding sites:
including: PWM scores, DNase (or ATAC) bins, 
and measured TF occupancy (from ChIP-seq data).

You can follow this [pipeline](./doc/preprocessing.html) to prepare training data.

<!-- # obtain candidate binding sites and get DNase (or ATAC) bins. Feel free to use your own pipelines -->
<!-- to obtain matrices of DNase (or ATAC) cleavage  -->
<!-- (and normalize by library size, etc.), and use our function  -->
<!-- `millipede_bin_dnase` or `MILLIPEDE` software to bin DNase (or ATAC) data  -->
<!-- with `MILLIPEDE` binning scheme. -->

<!-- You will also need to normalize ChIP-seq read counts across ChIP-seq experiments -->
<!-- (by library size or other methods).  -->

**Step 2: Assemble training data for all TF-cell type combinations**

Assemble training datasets for all training TF-cell type 
combinations, then split training data into 10 partitions.

```{r assemble-TOP-training-data, eval = FALSE}
set.seed(123)
max_sites <- 50000
total_partitions <- 10
max_sites_partition <- ceiling(max_sites/total_partitions)
training_chrs <- paste0('chr', seq(1,21,2))
training_data_name <- 'TOP_training_data'

# Create a directory to save training data
dir.create(training_data_dir, showWarnings = FALSE, recursive = TRUE)

for (k in 1:10) {
  training_data <- assemble_TOP_training_data(tf_cell_table,
                                              transform = 'asinh',
                                              training_chrs = training_chrs,
                                              sites_limit = max_sites_partition,
                                              total_partitions = total_partitions,
                                              n_part = k)
  # Add TF and cell type indices
  training_data$tf_id <- as.integer(factor(training_data$tf_name, levels = tf_list))
  training_data$cell_id <- as.integer(factor(training_data$cell_type, levels = celltype_list))
  saveRDS(training_data, 
          file.path(training_data_dir, paste0(training_data_name, '.partition', k, '.rds')))
}

# Save a table with all TF and cell type combinations
tf_cell_combos <- unique(training_data[, c('tf_id', 'cell_id', 'tf_name', 'cell_type')])
cat(nrow(tf_cell_combos), 'TF x cell type combos. \n')
write.table(tf_cell_combos, 
            file.path(training_data_dir, paste0(training_data_name, '_tf_cell_combos.txt')),
            sep = '\t', col.names = TRUE, row.names = FALSE, quote = FALSE)

```


Train TOP models using assembled training data
----------------------------------------------

Fit TOP model for each partition separately, and save the posterior samples.

We can set the following parameters for Gibbs sampling: 

  - n.iter: number of total iterations per chain (including burn-in iterations).
  - n.burnin: number of burn-in iterations, i.e. number of iterations to discard at the beginning.
  - n.chains: number of Markov chains.
  - n.thin: thinning rate, must be a positive integer.

To save computation time, we can run these 10 partitions in parallel 
on separate compute nodes (if you have access to compute clusters). 
```{r fit-TOP-model, eval = FALSE}
# Load TOP model
model.file <- '../model/TOP_M5_model_jags_priorVar1.txt'
# Create a directory to save results
dir.create(top_model_dir, showWarnings = FALSE, recursive = TRUE)

# Fit TOP model for each partition separately
# To save computation time, we can run these 10 partitions in parallel 
# on separate compute nodes instead of using the for loop. 
for (k in 1:10) {
  # Load training data for each partition
  data <- readRDS(file.path(training_data_dir, paste0(training_data_name, '.partition', k, '.rds')))
  # Print TFs and cell types included in the training data
  cat('Training TFs: ', levels(data$tf_name), '\n')
  cat('Training cell types: ', levels(data$cell_type), '\n')
  # Run Gibbs sampling to get TOP posterior samples
  top_samples <- fit_TOP_M5_model(data, model.file, n.iter=10000, n.burnin=5000, n.chains=3, n.thin=10)
  # Save TOP posterior samples for this partition
  saveRDS(top_samples, paste0(top_model_dir, '/TOP_M5_fit_partition', k, '.posterior_samples.rds'))
}
```

Combine TOP posterior samples from partitions and obtain the posterior mean of the regression coefficients
-----------------------------------------------------------------------------------------------------------

After the training is done, we can combine the posterior samples 
from the 10 partitions, and obtain the posterior mean of the regression coefficients.

We will use them to make predictions for TF occupancy using new DNase- or ATAC-seq data,
see [page](./doc/predict_TF_occupancy_with_trained_model.html)
```{r combine-TOP-coef, eval = FALSE}
# Combine TOP posterior samples from the 10 partitions and save the combined posterior samples
TOP_samples_files <- file.path(top_model_dir, paste0('TOP_M5_fit_partition', 1:10, '.posterior_samples.rds'))
TOP_samples <- combine_TOP_samples(TOP_samples_files, n_samples = n_samples)
# load the table with all TF and cell type combinations
tf_cell_combos <- read.table(tf_cell_combo_file, header=TRUE, sep='\t', stringsAsFactors = FALSE)
tf_cell_combos <- unique(tf_cell_combos[, c('tf_id', 'cell_id', 'tf_name', 'cell_type')])
# Extract posterior mean coefficients for all three levels
TOP_mean_coef <- extract_TOP_mean_coef(TOP_samples, tf_cell_combos)
# Save posterior samples and posterior mean of the regression coefficients
saveRDS(TOP_samples, file.path(top_model_dir, 'TOP_M5_combined_posterior_samples.rds'))
saveRDS(TOP_mean_coef, file.path(top_model_dir, 'TOP_M5_posterior_mean_coef.rds'))
```

We will provide pre-trained models using ENCODE data
[here](https://users.cs.duke.edu/~amink/software/).. 

