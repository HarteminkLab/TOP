---
title: 'Train TOP quantitative occupancy model'
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Train TOP quantitative occupancy model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = '#>'
)
```

TOP jointly fits a Bayesian hierarchical model using available training data 
from many TFs in many cell types. 

Training TOP entailed two basic steps, as illustrated in the figure below: 

```{r TOP-diagram, echo=FALSE, out.width = '90%'}
knitr::include_graphics("../man/figures/schematic.png")
```

First, following the site-centric strategy, we used motif matches to 
enumerate candidate binding sites, and extracted DNase- and/or 
ATAC-seq and ChIP data centered on each site for training. 

Second, we fit our Bayesian hierarchical regression model on 
spatially-binned DNase- and/or ATAC-seq data. 

Once TOP is trained, we can use it to predict occupancy 
for cell types or conditions for 
which we have DNase- or ATAC-seq data, 
generating TF occupancy estimates for
ChIP-seq experiments that have not been done.


Prepare training data
---------------------

As we learn the parameters from many TFs in many cell types together, 
it takes some efforts to prepare the training data. 

**Step 1: Prepare training data for each TF in each cell type**

Firstly, we need to prepare training data
for each training TF x cell type combination, and save the training data files 
(as `.rds` files in the `data_file` column in the table below).

This [page](data_preparation.html) shows the procedure to prepare the training data.
Basically, for each TF in each cell type, 
we prepare training data for candidate binding sites:
including: PWM scores of the motif matches, 
DNase (or ATAC) counts in MILLIPEDE bins, 
and measured TF occupancy (from ChIP-seq data).

**Step 2: Assemble training data for all TF x cell type combinations**

We create a table (data frame) listing all training TF x cell type 
combinations. The table should have three columns:
TF names, cell types, and paths to the training data files, like:

|   tf_name    |   cell_type   |        data_file         |
|:------------:|:-------------:|:------------------------:|
|     CTCF     |     K562      |   CTCF.K562.data.rds     |
|     CTCF     |     A549      |   CTCF.A549.data.rds     |
|     CTCF     |    GM12878    |   CTCF.GM12878.data.rds  |
|     NRF1     |     K562      |   NRF1.K562.data.rds     |
|     MYC      |     K562      |   MYC.K562.data.rds      |
|     ...      |     ...       |   ...                    |

In the example below, we split the training data randomly into 10 equal partitions, 
so that we could run Gibbs sampling on these partitions separately in parallel to
reduce the running time. 

We choose the training chromosomes by specifying the chromosomes in 
`training_chrs`, for example using odd chromosomes as below.
The chromosome names ("chr...") should match with those in the training data.

```{r load-TOP-package, eval=FALSE}
library(TOP)
```

```{r assemble-TOP-training-data, eval=FALSE}
assembled_training_data <- assemble_training_data(tf_cell_table,
                                                  logistic_model = FALSE, # do not use logistic model
                                                  chip_colname = 'chip', # name of the column with ChIP occupancy in training data
                                                  training_chrs = paste0('chr', seq(1,21,2)), 
                                                  n_partitions = 10)
```

Fit TOP quantitative occupancy models using assembled training data
--------------------------------------------------------------------

Here, we fit TOP quantitative occupancy models with ChIP-seq read counts. 
To fit TOP logistic models with ChIP-seq binary labels, 
please follow this [tutorial](train_TOP_logistic_model.htmll).

We use the consensus Monte Carlo algorithm to run 
Gibbs sampling on each of the 10 training data partitions separately in parallel.

We fit TOP models using [JAGS](https://mcmc-jags.sourceforge.io), 
and we uses the [R2jags](https://cran.r-project.org/web/packages/R2jags/) 
R package to call JAGS from R.

Please install `R2jags` if you need to train your own TOP models. 
```{r load-R2jags, eval=FALSE}
library(R2jags)
```

We provided the TOP model files in JAGS code 
in the `model` directory within the package. 
It is flexible (and easy) to modify the models, 
e.g. use different priors, different bins, or other genomic features.  
```{r TOP-model-file, eval=FALSE}
model_file <- system.file("model", "TOP_M5_model.jags", package = "TOP")

# show the model
file.show(model_file)
```

The `fit_TOP_model()` function below runs Gibbs sampling for each of the 10 partitions 
in parallel (using the `foreach` R package). 

We first perform "asinh" transform to the training ChIP-seq counts,
then fit the TOP quantitative occupancy model for each of the 10 partitions in parallel.

Set the following parameters for Gibbs sampling: 

- n_iter: number of total iterations per chain (including burn-in iterations).
- n_burnin: number of burn-in iterations, i.e. number of iterations to discard at the beginning.
- n_chains: number of Markov chains.
- n_thin: thinning rate, must be a positive integer.

The following example runs 5000 iterations of Gibbs sampling in total, 
including 1000 burn-ins, with 3 Markov chains, at a thinning rate of 2, 
and save the posterior samples to the `TOP_fit` directory. 

It could take a long time if we have many TFs and many cell types
in the training data.

```{r fit-TOP-model, eval=FALSE}
all_TOP_samples <- fit_TOP_M5_model(assembled_training_data, 
                                    model_file = model_file,
                                    logistic_model = FALSE,
                                    transform = 'asinh', 
                                    partitions = 1:10,
                                    n_iter = 5000,
                                    n_burnin = 1000,
                                    n_chains = 3,
                                    n_thin = 2,
                                    save = TRUE,
                                    out_dir='TOP_fit',
                                    quiet = TRUE)
```

This fits a TOP model on all 10 partitions in parallel on 10 CPU cores, 
and returns a list of posterior samples of the coefficients 
for each of the 10 partitions.
It requires a compute node/machine with 10 cores and 
may require a big memory if you have training data from 
many TFs and cell types. 
Alternatively, if you have limited computing resource, 
you may fit model for each of the 10
the partitions on separate machines, by
specifying the partition to run. 
For example, setting argument `partitions=3` 
will fit TOP model for training data in the 3rd partition.  

Combine TOP posterior samples from all partitions and extract the posterior mean of the regression coefficients
----------------------------------------------------------------------------------------------------------------

After we finished Gibbs sampling for all 10 partitions, 
we select and combine the posterior samples from all the partitions.
```{r combine-TOP-samples, eval=FALSE}
TOP_samples <- combine_TOP_samples(all_TOP_samples)
dim(TOP_samples)
```

We can extract posterior mean of the coefficients for all three levels.
```{r extract-TOP-mean-coef, eval=FALSE}
TOP_mean_coef <- extract_TOP_mean_coef(TOP_samples, 
                         assembled_training_data = assembled_training_data)
```

Save the posterior samples and posterior mean coefficients for predictions.
```{r save-samples-coef, eval=FALSE}
saveRDS(TOP_samples, 'TOP_fit/TOP_M5_combined_posterior_samples.rds')
saveRDS(TOP_mean_coef, 'TOP_fit/TOP_M5_posterior_mean_coef.rds')
```

We will provide pre-trained models using ENCODE data
[here](https://users.cs.duke.edu/~amink/software/).

To make predictions for TF occupancy using new DNase- or ATAC-seq data 
using trained model coefficients,
please follow this tutorial 
["Predict TF occupancy using trained TOP model"](predict_TF_occupancy_with_trained_model.html).

