---
title: "Prepare input data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Prepare input data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

TOP input data
--------------
TOP requires a data frame as input data for each TF in each cell type. 

The format of the input data frame:

- The first six columns: chr, start, end, site name, strand, motif PWM score.
- The next five columns (if using M5 bins): five MILLIPEDE bins around motif matches. 
- optional: one ChIP column as the response variable. 
It could be quantitative TF occupancy 
(asinh transformed ChIP-seq read counts) or 
binary TF binding labels (from ChIP-seq peaks).


Here we give an example for preparing input data for TOP to predict CTCF occupancy 
in K562 cell type.

Major steps
--------------

**Step 1: Find TF motif matches using FIMO software**

To scan for TF motif matches, we use the [FIMO](http://meme-suite.org/doc/fimo.html?man_type=web)
software from the MEME suite.

We use the command line version of FIMO. By default, we used 
with the threshold `p-value < 1e-5` and uniform background nucleotide frequency. 
You can use FIMO's default threshold `p-value < 1e-4`
(which will result in more motif matches), or use other background nucleotide frequency.

FIMO command line: `fimo [options] <motif file> <sequence file>`.

  - motif file: The name of a file containing MEME formatted motifs. 
  - sequence file: The name of a file containing a collection of sequences in FASTA format.

Download the CTCF motif file `MA0139.1.meme` (in MEME format)
from [JASPAR](https://jaspar.genereg.net/matrix/MA0139.1/). 

Download hg38 fasta file and save as `hg38.fa`.
```{bash download-fasta, eval=FALSE}
wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/analysisSet/hg38.analysisSet.fa.gz
gunzip hg38.analysisSet.fa.gz
mv hg38.analysisSet.fa hg38.fa
```

Generate the chrom.sizes file which will be needed later.
```{bash get-chrom-sizes, eval = FALSE}
samtools faidx hg38.fa
cut -f 1,2 hg38.fa.fai > hg38.chrom.sizes
```

Save FIMO output in text format (`MA0139.1_1e-5.fimo.txt`), 
which will be used in the next step. 

```{bash get-motif-matches, eval = FALSE}
# Find motif matches for CTCF
fimo --text \
     --skip-matched-sequence \
     --verbosity 2 \
     --bgfile --uniform-- \
     --thresh 1e-5 \
     --max-stored-scores 1000000 \
     MA0139.1.meme hg38.fa \
     > MA0139.1_1e-5.fimo.txt
```

**Step 2: Get candidate TF binding sites**

We take motif matches obtained from FIMO (`MA0139.1_1e-5.fimo.txt`) 
as candidate binding sites, and add 100 bp flanking regions on 
both sides of the motifs, then filter candidate sites by FIMO p-value, and
filter the candidate sites falling in ENCODE blacklist regions. 
We save the candidate sites in a text file (`MA0139.1_1e-5.candidate_sites.txt`)

Download ENCODE blacklist from [ENCODE portal](https://www.encodeproject.org/annotations/ENCSR636HFF/)

Load TOP R package
```{r load-TOP-package, eval = FALSE, message=FALSE, warning=FALSE}
library(TOP)
```

```{r get-candidate-sites, eval = FALSE}
sites.df <- process_candidate_sites(fimo_file='MA0139.1_1e-5.fimo.txt', 
                                    flank=100, 
                                    thresh_pValue=1e-5, 
                                    blacklist_file='blacklist.hg38.bed.gz',
                                    out_file = 'processed_data/MA0139.1_1e-5.candidate_sites.txt')
```

**Step 3: Count DNase- or ATAC-seq genome-wide cleavage**

We use ATAC-seq reads in K562 cell line from ENCODE (ID: `ENCSR868FGK`) for example. 

There are three replicates in this study. In practice, we merge replicate samples.
Here we only use one of the replicates (`ENCFF534DCE.bam`) as an example 
to demo the process, and rename the file as `K562.ATAC.bam`.

We first sort and index the BAM file using `samtools`,
and then print the stats of the reads, which will be used later 
when normalizing read counts by library sizes.

```{bash cmd-sort-index-stats-ATAC-bam, eval = FALSE}
# rename the bam file
mv ENCFF534DCE.bam K562.ATAC.bam

# This bam file is already sorted, so we skip the sorting step. 
samtools index K562.ATAC.bam
samtools idxstats K562.ATAC.bam > K562.ATAC.bam.idxstats.txt
```

Next, we count the cleavages along the genome, 
and save in Bigwig format. 
This step takes more time but only needs to be done once, 
and allows us to efficiently extract the read counts around candidate sites
for many different motifs.

We need `bedtools` and `bedGraphToBigWig` from UCSC for this step.
```{r count_genome_coverage, eval = FALSE}
# bam_file: input BAM filename.
# chrom_size_file: chrom sizes file.
# outdir: Save bigWig files of genome counts in this directory.
count_genome_cuts(bam_file = 'K562.ATAC.bam', 
                  chrom_size_file = 'hg38.chrom.sizes', 
                  outdir = "processed_data/")
```

**Step 4: Get DNase- or ATAC-seq count matrices for each motif, then normalize, bin and transform the counts**

Get count matrices around candidate sites.
We need `bwtool` for this step.
```{r get-motif-counts, eval = FALSE}
# sites_file: Candidate sites filename.
# genomecount_dir: Genome counts directory.
# genomecount_name: Genome counts prefix.
count_matrix <- get_sites_counts(sites_file='processed_data/MA0139.1_1e-5.candidate_sites.txt', 
                                 genomecount_dir='./processed_data/',
                                 genomecount_name='K562.ATAC')

```

Normalize, bin and transform counts.
```{r normalize-bin-counts, eval = FALSE}
# count_matrix: ATAC (or DNase) count matrix
# idxstats_file: The idxstats file generated by samtools
# ref.size: Scale to reference library size 
# (default: 50 million for ATAC-seq, 100 million for DNase-seq)
# bin.method: MILLIPEDE binning scheme (default: 'M5').
# transform: asinh transform of the counts.
bins.df <- normalize_bin_transform_counts(count_matrix, 
                                          idxstats_file='K562.ATAC.bam.idxstats.txt', 
                                          ref.size=5e7,
                                          bin.method='M5',
                                          transform='asinh')
```

Make a data frame with PWM scores, and ATAC (or DNase) bins. 
We can apply TOP model on this data frame to make predictions.
```{r combine-sites-bins, eval=FALSE}
combined_data.df <- data.frame(sites.df, bins.df)
colnames(combined_data.df) <- c('chr','start','end','name','pwm.score','strand','p.value', paste0('bin', 1:ncol(bins.df)))

saveRDS(combined_data.df, 'processed_data/CTCF_MA0139.1_1e-5.K562.ATAC.M5.combined.data.rds')
```

```{r load-combined-data, eval=TRUE}
combined_data.df <- readRDS('../inst/example/processed_data/CTCF_MA0139.1_1e-5.K562.ATAC.M5.combined.data.rds')
head(combined_data.df, 3)
```

**Step 5: Prepare ChIP-seq data (optional if you want to train your own model)**

Download CTCF K562 ChIP-seq bam files (ENCODE ID: `ENCSR000EGM`, two replicates: `ENCFF172KOJ` and `ENCFF265ZSP`).
We again sort and index the BAM files and print the stats of the reads using `samtools`.

```{bash cmd-sort-index-stats-chip-bam, eval = FALSE}
# rename the bam files
mv ENCFF172KOJ.bam CTCF.K562.ChIPseq.rep1.bam
mv ENCFF265ZSP.bam CTCF.K562.ChIPseq.rep2.bam

# The bam files are already sorted, so we skip the sorting step. 
samtools index CTCF.K562.ChIPseq.rep1.bam
samtools idxstats CTCF.K562.ChIPseq.rep1.bam > CTCF.K562.ChIPseq.rep1.bam.idxstats.txt

samtools index CTCF.K562.ChIPseq.rep2.bam
samtools idxstats CTCF.K562.ChIPseq.rep2.bam > CTCF.K562.ChIPseq.rep2.bam.idxstats.txt

```

Count ChIP-seq reads around candidate sites (merge ChIP-seq replicates),
and normalize (scale) to the same reference library size (10 million).
```{r count-normalize-chip, eval=FALSE}
# ref.size: ChIP-Seq reference library size (default: 10 million)
sites_chip.df <- count_normalize_chip(sites_file = 'processed_data/MA0139.1_1e-5.candidate_sites.txt', 
                                      chip_bam_files = c('CTCF.K562.ChIPseq.rep1.bam', 'CTCF.K562.ChIPseq.rep2.bam'),
                                      chrom_size_file = 'hg38.chrom.sizes', 
                                      ref.size = 1e7)
```

Combine PWM scores, ATAC (or DNase) bins, and ChIP-seq counts into a data frame.
```{r combine-sites-bins-chip, eval=FALSE}
combined_data.df <- data.frame(sites.df, bins.df, sites_chip.df$chip)
colnames(combined_data.df) <- c('chr','start','end','name','pwm.score','strand','p.value', paste0('bin', 1:ncol(bins.df)), 'chip')
saveRDS(combined_data.df, 'processed_data/CTCF_MA0139.1_1e-5.K562.ATAC.M5.ChIP.combined.data.rds')
```

```{r load-combined-data-with-chip, eval=TRUE}
combined_data.df <- readRDS('../inst/example/processed_data/CTCF_MA0139.1_1e-5.K562.ATAC.M5.ChIP.combined.data.rds')
head(combined_data.df, 3)
```

Replace ChIP-seq counts with binary ChIP labels (from ChIP-seq peaks) 
if you want to train TOP logistic version to predict TF binding probability.

Snakemake pipeline
-------------------

We provided [Snakemake pipelines](https://github.com/kevinlkx/TOP/tree/main/inst/snakemake) 
to automate the whole process using the above mentioned R scripts. 
The Snakemake is especially helpful if you have many TFs (motifs) in many cell types.

Run `Snakefile_training_ATAC` for ATAC-seq, or `Snakefile_training_DNase` for DNase-seq. 
For more details and instructions about running Snakemake pipelines, 
see [Snakemake tutorial](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html).
