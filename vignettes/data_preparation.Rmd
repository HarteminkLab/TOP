---
title: "Prepare input data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Prepare input data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

Input data
--------------
TOP requires a data frame for each TF and cell type of interest. 

The format of the input data frame:

- Columns of the candidate binding sites: 
chr, start, end, site name, PWM score, strand, p-value from (FIMO) motif match scanning. 

- Columns of DNase-seq or ATAC-seq bins:
MILLIPEDE bins around motif matches (default: M5 bins). 

- optional: one column of ChIP-seq measurement, if you want to train the models.
It could be quantitative TF occupancy 
(asinh transformed ChIP-seq read counts) or 
binary TF binding labels (from ChIP-seq peaks).

Example procedure
-----------------

Here are example steps of preparing input data for TOP 
to predict CTCF occupancy in K562 cell type:


**Step 1: Find TF motif matches using FIMO software**

To scan for TF motif matches, we use the [FIMO](http://meme-suite.org/doc/fimo.html?man_type=web)
software from the MEME suite.

We use the command line version of FIMO. By default, we used 
with the threshold `p-value < 1e-5` and uniform background nucleotide frequency. 
You can use FIMO's default threshold `p-value < 1e-4`
(which will result in more motif matches), or use other background nucleotide frequency.

FIMO command line: `fimo [options] <motif file> <sequence file>`.

  - motif file: The name of a file containing MEME formatted motifs. 
  - sequence file: The name of a file containing a collection of sequences in FASTA format.

Download the CTCF motif file `MA0139.1.meme` (in MEME format)
from [JASPAR](https://jaspar.genereg.net/matrix/MA0139.1/). 

Download hg38 fasta file and save as `hg38.fa`.
```{bash download-fasta, eval=FALSE}
wget https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/analysisSet/hg38.analysisSet.fa.gz
gunzip hg38.analysisSet.fa.gz
mv hg38.analysisSet.fa hg38.fa
```

Generate the chrom.sizes file which will be needed later.
```{bash get-chrom-sizes, eval = FALSE}
samtools faidx hg38.fa
cut -f 1,2 hg38.fa.fai > hg38.chrom.sizes
```

Save FIMO output in text format (`MA0139.1_1e-5.fimo.txt`), 
which will be used in the next step. 

```{bash get-motif-matches, eval = FALSE}
# Find motif matches for CTCF
fimo --text \
     --skip-matched-sequence \
     --verbosity 2 \
     --bgfile --uniform-- \
     --thresh 1e-5 \
     --max-stored-scores 1000000 \
     MA0139.1.meme hg38.fa \
     > MA0139.1_1e-5.fimo.txt
```

**Step 2: Get candidate TF binding sites**

We take motif matches obtained from FIMO (`MA0139.1_1e-5.fimo.txt`) 
as candidate binding sites, and add 100 bp flanking regions on 
both sides of the motifs, then filter candidate sites by FIMO p-value, and
filter the candidate sites falling in ENCODE blacklist regions. 
We save the candidate sites in a text file (`MA0139.1_1e-5.candidate_sites.txt`)

Download ENCODE blacklist from [ENCODE portal](https://www.encodeproject.org/annotations/ENCSR636HFF/)

Load TOP R package
```{r load-TOP-package, eval = FALSE, message=FALSE, warning=FALSE}
library(TOP)
```

```{r get-candidate-sites, eval = FALSE}
sites.df <- process_candidate_sites(fimo_file='MA0139.1_1e-5.fimo.txt', 
                                    flank=100, 
                                    thresh_pValue=1e-5, 
                                    blacklist_file='blacklist.hg38.bed.gz',
                                    out_file = 'processed_data/MA0139.1_1e-5.candidate_sites.txt')
```

**Step 3: Count DNase- or ATAC-seq genome-wide cleavage**

We use ATAC-seq reads in K562 cell line from ENCODE (ID: `ENCSR868FGK`) for example. 

There are three replicates in this study. In practice, we merge replicate samples.
Here we only use one of the replicates (`ENCFF534DCE.bam`) as an example 
to demo the process, and rename the file as `K562.ATAC.bam`.

We first sort and index the BAM file using `samtools`,
and then print the stats of the reads, which will be used later 
when normalizing read counts by library sizes.

```{bash cmd-sort-index-stats-ATAC-bam, eval = FALSE}
# rename the bam file
mv ENCFF534DCE.bam K562.ATAC.bam

# This bam file is already sorted, so we skip the sorting step. 
samtools index K562.ATAC.bam
samtools idxstats K562.ATAC.bam > K562.ATAC.bam.idxstats.txt
```

Next, we count the cleavages along the genome, 
and save in Bigwig format. 
This step takes more time but only needs to be done once, 
and allows us to efficiently extract the read counts around candidate sites
for many different motifs.

We need `bedtools` and `bedGraphToBigWig` from UCSC for this step.
```{r count_genome_coverage, eval = FALSE}
# bam_file: input BAM filename.
# chrom_size_file: chrom sizes file.
# outdir: Save bigWig files of genome counts in this directory.
count_genome_cuts(bam_file = 'K562.ATAC.bam', 
                  chrom_size_file = 'hg38.chrom.sizes', 
                  outdir = "processed_data/")
```

**Step 4: Get DNase- or ATAC-seq count matrices for each motif, then normalize, bin and transform the counts**

Get count matrices around candidate sites.
We need `bwtool` for this step.
```{r get-motif-counts, eval = FALSE}
# sites_file: Candidate sites filename.
# genomecount_dir: Genome counts directory.
# genomecount_name: Genome counts prefix.
count_matrix <- get_sites_counts(sites_file='processed_data/MA0139.1_1e-5.candidate_sites.txt', 
                                 genomecount_dir='./processed_data/',
                                 genomecount_name='K562.ATAC')

```

Normalize, bin and transform counts.
```{r normalize-bin-counts, eval = FALSE}
# count_matrix: ATAC (or DNase) count matrix
# idxstats_file: The idxstats file generated by samtools
# ref.size: Scale to reference library size 
# (default: 50 million for ATAC-seq, 100 million for DNase-seq)
# bin.method: MILLIPEDE binning scheme (default: 'M5').
# transform: asinh transform of the counts.
bins.df <- normalize_bin_transform_counts(count_matrix, 
                                          idxstats_file='K562.ATAC.bam.idxstats.txt', 
                                          ref.size=5e7,
                                          bin.method='M5',
                                          transform='asinh')
```

Make a data frame with PWM scores, and ATAC (or DNase) bins. 
We can apply TOP model on this data frame to make predictions.
```{r combine-sites-bins, eval=FALSE}
combined_data.df <- data.frame(sites.df, bins.df)
colnames(combined_data.df) <- c('chr','start','end','name','pwm.score','strand','p.value', paste0('bin', 1:ncol(bins.df)))

saveRDS(combined_data.df, 'processed_data/CTCF_MA0139.1_1e-5.K562.ATAC.M5.combined.data.rds')
```

```{r load-combined-data, eval=TRUE}
combined_data.df <- readRDS('../inst/example/processed_data/CTCF_MA0139.1_1e-5.K562.ATAC.M5.combined.data.rds')
head(combined_data.df, 3)
```

**Step 5: Prepare ChIP-seq data (optional if you want to train your own model)**

Download CTCF K562 ChIP-seq bam files (ENCODE ID: `ENCSR000EGM`, two replicates: `ENCFF172KOJ` and `ENCFF265ZSP`).
We again sort and index the BAM files and print the stats of the reads using `samtools`.

```{bash cmd-sort-index-stats-chip-bam, eval = FALSE}
# rename the bam files
mv ENCFF172KOJ.bam CTCF.K562.ChIPseq.rep1.bam
mv ENCFF265ZSP.bam CTCF.K562.ChIPseq.rep2.bam

# The bam files are already sorted, so we skip the sorting step. 
samtools index CTCF.K562.ChIPseq.rep1.bam
samtools idxstats CTCF.K562.ChIPseq.rep1.bam > CTCF.K562.ChIPseq.rep1.bam.idxstats.txt

samtools index CTCF.K562.ChIPseq.rep2.bam
samtools idxstats CTCF.K562.ChIPseq.rep2.bam > CTCF.K562.ChIPseq.rep2.bam.idxstats.txt

```

Count ChIP-seq reads around candidate sites (merge ChIP-seq replicates),
and normalize (scale) to the same reference library size (10 million).
```{r count-normalize-chip, eval=FALSE}
# ref.size: ChIP-Seq reference library size (default: 10 million)
sites_chip.df <- count_normalize_chip(sites_file = 'processed_data/MA0139.1_1e-5.candidate_sites.txt', 
                                      chip_bam_files = c('CTCF.K562.ChIPseq.rep1.bam', 'CTCF.K562.ChIPseq.rep2.bam'),
                                      chrom_size_file = 'hg38.chrom.sizes', 
                                      ref.size = 1e7)
```

Combine PWM scores, ATAC (or DNase) bins, and ChIP-seq counts into a data frame.
```{r combine-sites-bins-chip, eval=FALSE}
combined_data.df <- data.frame(sites.df, bins.df, sites_chip.df$chip)
colnames(combined_data.df) <- c('chr','start','end','name','pwm.score','strand','p.value', paste0('bin', 1:ncol(bins.df)), 'chip')
saveRDS(combined_data.df, 'processed_data/CTCF_MA0139.1_1e-5.K562.ATAC.M5.ChIP.combined.data.rds')
```

```{r load-combined-data-with-chip, eval=TRUE}
combined_data.df <- readRDS('../inst/example/processed_data/CTCF_MA0139.1_1e-5.K562.ATAC.M5.ChIP.combined.data.rds')
head(combined_data.df, 3)
```

Replace ChIP-seq counts with binary ChIP labels (from ChIP-seq peaks) 
if you want to train TOP logistic version to predict TF binding probability.

Snakemake pipeline
-------------------

It is recommended to write a [Snakemake pipeline](https://github.com/kevinlkx/TOP/tree/main/inst/snakemake) 
to automate the whole process. It is especially helpful if you have many TFs and many cell types.

For more details and instructions about running Snakemake pipelines, 
see [Snakemake tutorial](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html).
