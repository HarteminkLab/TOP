---
title: 'Train TOP logistic model'
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Train TOP logistic model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = '#>'
)
```

Load TOP R package
---------------------

```{r load-TOP-package, eval = FALSE}
library(TOP)
```


Prepare training data
---------------------

**Step 1: Prepare training data for each TF in each cell type**

For each TF in each cell type, prepare training data for candidate binding sites:
including: PWM scores, DNase (or ATAC) bins, 
and binary ChIP labels (from ChIP-seq peaks).

You can follow this [page](data_preparation.html) to prepare training data.

Create a tab delimited table (e.g. `tf_cell_table.txt`) 
listing all the TFs and cell types. 
The table should have three columns:
TF names, cell types, and paths to the training data files, like:

|   tf_name    |   cell_type   |        data_file       |
|:------------:|:-------------:|:----------------------:|
|     CTCF     |     K562      |   CTCF.K562.data.rds   |
|     CTCF     |     A549      |   CTCF.A549.data.rds   |
|     NRF1     |     K562      |   NRF1.K562.data.rds   |
|     ...      |     ...       |   ...                  |


**Step 2: Assemble training data for all TF-cell type combinations**

Assemble training datasets for all training TF-cell type 
combinations, then split training data into 10 partitions.

```{r assemble-TOP-logistic-training-data, eval = FALSE}
all_training_data <- assemble_TOP_training_data(tf_cell_table_file = 'tf_cell_table.txt',
                                                logistic.model = TRUE,
                                                chip_colname = 'chip_label',
                                                training_chrs = paste0('chr', seq(1,21,2)), 
                                                n.partitions = 10)
```


Train TOP logistic models using assembled training data
-------------------------------------------------------------
Train TOP logistic model for each partition separately, and save the posterior samples.

We can set the following parameters for Gibbs sampling: 

- n.iter: number of total iterations per chain (including burn-in iterations).
- n.burnin: number of burn-in iterations, i.e. number of iterations to discard at the beginning.
- n.chains: number of Markov chains.
- n.thin: thinning rate, must be a positive integer.

This step usually takes a long time if we have many TFs and many cell types. 

To save computation time, we can run the partitions 
(choose which partition to run, e.g. `partitions=3`) 
in parallel on separate compute nodes (if you have access to compute clusters). 

**Note**: we need to install `R2jags` and/or `rjags` packages to fit TOP models 
in our current version using `JAGS`.

```{r train-TOP-logistic-model, eval = FALSE}
library(R2jags)

TOP_samples_files <- fit_TOP_model(all_training_data, 
                                   model.file = '../model/TOP_M5_logit_model_jags_priorVar1.txt',
                                   logistic.model = TRUE,
                                   out.dir = 'TOP_logistic_fit',
                                   partitions = 1:10,
                                   n.iter = 5000, 
                                   n.burnin = 2500,
                                   n.chains = 3,
                                   n.thin = 10)
```


Combine TOP posterior samples from all partitions and extract the posterior mean of the regression coefficients
----------------------------------------------------------------------------------------------------------------

After the training is done, we can combine the posterior samples from the partitions,
and obtain the posterior mean of the regression coefficients.

Combine TOP posterior samples from the 10 partitions
```{r combine-TOP-coef, eval = FALSE}
TOP_samples <- combine_TOP_samples(TOP_samples_files)
```

Extract posterior mean coefficients for all three levels
```{r extract-TOP-mean-coef, eval = FALSE}
TOP_mean_coef <- extract_TOP_mean_coef(TOP_samples, tf_cell_combos)
```

Save posterior samples and posterior mean of the regression coefficients
```{r save-samples-coef, eval = FALSE}
saveRDS(TOP_samples, 'TOP_output/TOP_logistic_M5_combined_posterior_samples.rds')
saveRDS(TOP_mean_coef, 'TOP_output/TOP_logistic_M5_posterior_mean_coef.rds')
```

We will provide pre-trained models using ENCODE data
[here](https://users.cs.duke.edu/~amink/software/).

To make predictions for TF occupancy using new DNase- or ATAC-seq data,
see this [page](predict_TF_occupancy_with_trained_model.html)

